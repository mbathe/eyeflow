---
id: llm-calls
sidebar_position: 8
title: Appels LLM avanc√©s
description: CompiledLLMContext fig√©, pipeline multi-LLM, boucles de raisonnement born√©es, g√©n√©ration contrainte par logit_bias et r√©solution de slots dynamiques.
---

# Appels LLM avanc√©s

EyeFlow utilise les LLM de mani√®re **radicalement diff√©rente** des syst√®mes agents classiques : les appels LLM sont fig√©s √† la compilation et ex√©cut√©s de fa√ßon d√©terministe √† l'ex√©cution.

---

## CompiledLLMContext ‚Äî Contexte fig√©

√Ä la compilation, chaque instruction `LLM_CALL` re√ßoit un `CompiledLLMContext` complet qui est **s√©rialis√© dans le binaire** :

```typescript
interface LlmCompiledContext {
  // Mod√®le et param√®tres
  model:        string;    // "gpt-4o-2024-08-06" ‚Äî version exacte
  temperature:  number;    // Calibr√©e au type de t√¢che
  maxTokens:    number;
  
  // Contexte syst√®me
  systemPrompt: string;    // Prompt structur√© anti-hallucination
  
  // Exemples fig√©s
  fewShotExamples: LlmFewShotExample[];
  
  // Sch√©ma de sortie
  outputSchema: object;    // JSON Schema ‚Äî utilis√© pour logit_bias
  
  // Slots dynamiques (r√©solus √† l'ex√©cution)
  dynamicSlots: LlmDynamicSlot[];
  
  // Template de prompt optionnel
  promptTemplate?: string;
}
```

### Calibration automatique de la temp√©rature

Le compilateur calibre automatiquement `temperature` selon le type de t√¢che d√©tect√© :

| Type de t√¢che | Mots-cl√©s d√©tect√©s | Temperature |
|---------------|-------------------|-------------|
| Extraction | extrai*, lire, identifier | **0.0** |
| Validation | valid*, v√©rifie*, classifie* | **0.1** |
| Raisonnement | analyser, diagnostiquer, expliquer | **0.3** |
| G√©n√©ration | r√©dige*, cr√©er, g√©n√©rer | **0.7** |

---

## Few-Shot Examples fig√©s

Les exemples few-shot sont compil√©s dans le binaire et ne changent jamais √† l'ex√©cution :

```json
{
  "fewShotExamples": [
    {
      "label": "overpressure_detection",
      "inputJson": "{\"temp\": 145, \"pressure\": 8.2, \"level\": 0.91}",
      "outputJson": "{\"state\": \"overpressure\", \"action\": \"close_valve\"}"
    },
    {
      "label": "normal_state",
      "inputJson": "{\"temp\": 72, \"pressure\": 4.1, \"level\": 0.45}",
      "outputJson": "{\"state\": \"normal\", \"action\": \"log_only\"}"
    }
  ]
}
```

Ces exemples guident le LLM vers des sorties structur√©es coh√©rentes, **sans laisser place √† l'improvisation**.

---

## G√©n√©ration contrainte par logit_bias

EyeFlow utilise la fonctionnalit√© `logit_bias` des APIs LLM pour contraindre les tokens g√©n√©rables :

```typescript
// Construire le logit_bias depuis l'outputSchema
function buildLogitBias(schema: JSONSchema): Record<string, number> {
  const bias: Record<string, number> = {};
  
  if (schema.type === 'object') {
    // Forcer les tokens structurels JSON
    STRUCTURAL_TOKENS.forEach(t => bias[t] = 20);
  }
  
  if (schema.properties?.state?.enum) {
    // Seuls les tokens de l'enum sont autoris√©s pour ce champ
    const validTokens = schema.properties.state.enum
      .flatMap(v => tokenize(v));
    ALL_TOKENS
      .filter(t => !validTokens.includes(t))
      .forEach(t => bias[t] = -100);  // Interdit
  }
  
  return bias;
}
```

R√©sultat : le LLM ne peut **physiquement pas** g√©n√©rer une valeur hors domaine.

---

## Dynamic Slots ‚Äî R√©solution √† l'ex√©cution

Les `dynamicSlots` permettent d'injecter des donn√©es fra√Æches au moment de l'ex√©cution, **tout en maintenant le d√©terminisme** (le sch√©ma du slot est fig√© √† la compilation) :

```typescript
interface LlmDynamicSlot {
  slotId:     string;   // Cl√© dans le template de prompt
  sourceType: 'vault' | 'runtime';
  sourceKey:  string;   // Chemin Vault ou dot-notation runtime
}
```

### Slot depuis Vault (secrets)

```json
{
  "slotId": "api_endpoint",
  "sourceType": "vault",
  "sourceKey": "industrial/T-04/llm_endpoint"
}
```

La SVM appelle `VaultClient::fetch("industrial/T-04/llm_endpoint")` juste avant l'appel LLM.

### Slot depuis le contexte runtime

```json
{
  "slotId": "current_sensor_values",
  "sourceType": "runtime",
  "sourceKey": "event.payload.sensors"
}
```

La SVM extrait `event.payload.sensors` du contexte d'ex√©cution courant via dot-notation.

---

## Pipeline Multi-LLM

EyeFlow supporte le **cha√Ænage de plusieurs LLMs** dans un seul programme, chacun avec un r√¥le sp√©cifique :

```
Donn√©es brutes
      ‚îÇ
      ‚ñº LLM_CALL (Gemini Flash) ‚Äî extraction rapide bon march√©
      ‚îÇ { entity: "tank-T04", metric: "temp", value: 148.5 }
      ‚îÇ
      ‚ñº LLM_CALL (GPT-4o) ‚Äî raisonnement approfondi
      ‚îÇ { diagnosis: "thermal_expansion", severity: "high", root_cause: "..." }
      ‚îÇ
      ‚ñº LLM_CALL (Claude) ‚Äî r√©daction rapport
        { report: "Suite √† l'analyse thermique..." }
```

Configuration dans la r√®gle :

```json
{
  "llmPipeline": [
    {
      "step": "extraction",
      "model": "gemini-1.5-flash",
      "temperature": 0.0,
      "role": "fast_extraction"
    },
    {
      "step": "reasoning",
      "model": "gpt-4o-2024-08-06",
      "temperature": 0.3,
      "role": "deep_analysis",
      "inputFrom": "extraction"
    },
    {
      "step": "reporting",
      "model": "claude-3-5-sonnet",
      "temperature": 0.5,
      "role": "narrative_generation",
      "inputFrom": "reasoning"
    }
  ]
}
```

Les 3 mod√®les, leurs param√®tres et leurs exemples few-shot sont tous **fig√©s dans le binaire**.

---

## Boucles de raisonnement born√©es

Pour les t√¢ches qui n√©cessitent plusieurs it√©rations LLM (ex: r√©vision progressive d'un diagnostic), EyeFlow impose une borne stricte :

```protobuf
message LoopConfig {
  int32 max_iterations = 1;     // REQUIS ‚Äî refus√© par Z3 si absent
  string exit_condition = 2;    // Condition d'arr√™t pr√©coce
  bool   require_progress = 3;  // Arr√™t si pas d'am√©lioration
}
```

Z3 v√©rifie √† la compilation que toute boucle a un `max_iterations > 0`.

```json
{
  "loop": {
    "maxIterations": 3,
    "exitCondition": "diagnosis.confidence > 0.95",
    "requireProgress": true
  }
}
```

Si `requireProgress: true` et que deux it√©rations cons√©cutives produisent un r√©sultat identique ‚Üí la boucle s'arr√™te automatiquement.

---

## Prompt syst√®me anti-hallucination

Chaque appel LLM re√ßoit automatiquement ce prompt syst√®me structur√© :

```
Tu es un syst√®me d'analyse d√©terministe pour application critique.
R√®gles absolues :
1. R√©ponds UNIQUEMENT dans le format JSON sp√©cifi√©
2. N'invente JAMAIS de donn√©es non pr√©sentes dans l'input
3. Si tu n'es pas certain, utilise le champ "confidence" < 0.7
4. Ne g√©n√®re JAMAIS de valeurs hors des enums d√©finis
5. Chaque champ required DOIT √™tre pr√©sent dans ta r√©ponse
6. En cas d'ambigu√Øt√©, choisis la valeur la plus conservative (s√©curit√©)
7. N'ajoute JAMAIS de champs non d√©finis dans le sch√©ma
8. Ta r√©ponse doit √™tre parseable par JSON.parse() sans erreur
Input : {input_data}
```

---

## Prochaines √©tapes

üëâ [Compilation s√©mantique](./semantic-compilation) ‚Äî comment le contexte est inject√© pendant la compilation  
üëâ [SVM Runtime](./svm-runtime) ‚Äî r√©solution des slots √† l'ex√©cution  
üëâ [S√©curit√©](./security) ‚Äî mod√®le de menace contre l'injection de prompt
